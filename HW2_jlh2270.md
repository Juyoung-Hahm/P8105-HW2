HW2
================
Juyoung Hahm
10/1/2020

# Problem 1

Read and clean the Mr. Trash Wheel sheet:

``` r
TrashWheel = 
  read_excel("./Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
             sheet = "Mr. Trash Wheel", range = ("A2:N408")) %>%
  janitor::clean_names() %>%
  drop_na(dumpster) %>%
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

Read and clean precipitation data for 2017 and 2018. For each, omit rows
without precipitation data and add a variable `year`.

``` r
precip2017 = 
  read_excel("./Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
             sheet = "2017 Precipitation", range = ("A2:B14")) %>%
  janitor::clean_names() %>%
  mutate(year = 2017) %>%
  relocate(year)

precip2018 = 
  read_excel("./Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
             sheet = "2018 Precipitation", range = ("A2:B14")) %>%
  janitor::clean_names() %>%
  mutate(year = 2018) %>%
  relocate(year)
```

Next, combine precipitation data sets and convert month to a character
variable.

``` r
month_df = tibble(month = 1:12, month_name = month.name)
precip_df = bind_rows(precip2017, precip2018)
precip_df = left_join(precip_df, month_df, by = "month")
```

In the Mr. Trash Wheel data, `TrashWheel`, these data are recorded
starting from 2014 to 2019. It shows the weight of the trash(tons), and
kinds of trash, such as: `plastic_bottles`, `polystyrene`,
`cigarette_butts`, `glass_bottles`, `grocery_bags`, `chip_bags`,
`sports_balls`, `homes_powered`. Looking at each year’s data, we can see
that every number of trash have been increasing. For instance,

  - For the chip bags, on 2014, it was 141.35, and in 2018, it was
    310.39.

  - For the chip bags, on 2014, it was 70849, and in 2018, it was
    118980.

  - For the chip bags, on 2014, it was 82590, and in 2018, it was
    123800.

  - For the chip bags, on 2014, it was 4162000, and in 2018, it was
    803300.

According to the `precip_df`, the total precipitation in 2018 were
70.33. The median number of sports balls in a dumpster in 2017 were 8.

# Problem2

Read and clean the data; retain line, station, name, station latitude /
longitude, routes served, entry, vending, entrance type, and ADA
compliance.

``` r
nyc_subway = 
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>%
  select(line:route11, entry, vending, entrance_type, ada)
```

Next, we are going to convert `entry` into logical vector:

``` r
nyc_subway$entry = ifelse(nyc_subway$entry == "YES", T,F)
typeof(nyc_subway$entry)
```

    ## [1] "logical"

In this data `nyc_subway`, I first organized the variable name by
changing to lowercase, and ’\_’ to the spaces between the names, and
only removed variables except for these: `line`, `station_name`,
`station_latitude`, `station_longitude`, `route1` to `route11`, `entry`,
`vending`, `entrance_type`, `ada`.

After cleaning these variables, there were 35492 data. These data are
hard to read for the first-time seeing NYC subway because the station
names and lines are repeatedly shown. So, counting the distinct stations
would be helpful to see. According to `nyc_subway`:

  - There are 465 distinct stations in NYC.
  - 468 stations are ADA compliant.
  - The proportion of station entrances / exits without vending allow
    entrance are 183 to 69.

Below is the reformatted data, `nyc_subway_reformat` by creating new
variables, `route_name` and `route_number`.

``` r
nyc_subway_reformat = nyc_subway %>%
  mutate(
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11)
  ) %>%
  pivot_longer(
    route1:route11,
    names_to = "route_name",
    values_to = "route_number"
  )
```

  - There are 60 distinct stations that serve the A train.
  - There are 17 stations that serve the A train and ADA compliant.

# Problem 3

First, clean the data in pols-month.csv. Use separate() to break up the
variable mon into integer variables year, month, and day; replace month
number with month name

``` r
pols_month = 
  read_csv("./fivethirtyeight_datasets/pols-month.csv") %>%
  janitor::clean_names() %>%
  separate(mon, c("year", "month", "day"), sep = "-") %>%
  mutate(month = as.numeric(month))

month_df = tibble(month = 1:12, month_name = month.name) #used in Problem 1

pols_month = left_join(pols_month, month_df, by = "month")
pols_month =  pols_month %>%
  select(year, month_name, day:rep_dem)
```

Then,create a president variable taking values gop and dem, and remove
`prez_dem` and `prez_gop`; and remove the `day` variable.

``` r
pols_month = pols_month %>% 
  mutate(president = case_when(prez_gop == "1" ~ "gop", prez_dem == "1" ~ "dem")) %>%
  select(-prez_gop, -prez_dem, -day)
```

Then, clean the data in `snp` using a similar process to the above. For
consistency across datasets, arrange according to year and month, and
organize so that year and month are the leading columns.

``` r
snp = 
  read_csv("./fivethirtyeight_datasets/snp.csv") %>%
  janitor::clean_names() %>%
  separate(date, c("month", "day", "year"), sep = "/") %>%
  mutate(month = as.numeric(month))

month_df = tibble(month = 1:12, month_name = month.name) #used in Problem 1

snp = left_join(snp, month_df, by = "month")
snp =  snp %>%
  select(year, month_name, close)
```

Then, tidy the unemployment data so that it can be merged with the
previous datasets.

``` r
unemployment = 
  read_csv("./fivethirtyeight_datasets/unemployment.csv", col_types = ) %>%
  janitor::clean_names() 

unemployment = unemployment %>%
  pivot_longer(
    jan:dec,
    names_to = "month_name",
    values_to = "percentage_unemployment")
```

Then, create a data, `pols_snp_unemployment` by joining the datasets by
merging `snp` into `pols`, and merging `unemployment` into the result.

``` r
pols_snp_unemployment = merge(unemployment, merge(snp, pols_month))
```

In the `pols_month`, it showed the number of national politicians who
are democratic or republican at any given time. `snp`showes a
representative measure of stock market as a whole. `unemployment` showed
each month’s unemployment percentages. In order to merge all three data,
we separated a column `month_name` in order to merge the datasets
easily.

After merging the data, there are 7920 data. The range is 1950, 2015.
The main variables we need to look closely are: `year`,
`percentage_unemployment` and `president`.

We can know that the president from certain party had a lowest or highes
unemployemnt rate. With this data, we can establish and predict which
party had effected positively on our nation.
